{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(r\"train.csv\")\n",
    "test = pd.read_csv(r\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train과 y_train 설정\n",
    "X_train = train.drop(columns=['1005004000_velocity'])\n",
    "y_train = train[['1005004000_velocity']]\n",
    "\n",
    "# X_test와 y_test 설정\n",
    "X_test = test.drop(columns=['1005004000_velocity'])\n",
    "y_test = test[['1005004000_velocity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (19261, 68)\n",
      "y_train shape: (19261, 1)\n",
      "X_test shape: (696, 68)\n",
      "y_test shape: (696, 1)\n"
     ]
    }
   ],
   "source": [
    "# 타겟 컬럼과 시간 시차 설정\n",
    "TARGET = '1005004000_velocity'\n",
    "HORIZON = 24  # 24시간 시차\n",
    "\n",
    "# 24시간 시차 적용하여 데이터 생성\n",
    "train['y_shifted'] = train[TARGET].shift(-HORIZON)  # 24시간 이후 값을 타겟으로 설정\n",
    "test['y_shifted'] = test[TARGET].shift(-HORIZON)\n",
    "\n",
    "# 비어 있는 데이터 제거\n",
    "train = train.dropna(subset=['y_shifted'])\n",
    "test = test.dropna(subset=['y_shifted'])\n",
    "\n",
    "# X_train, y_train 설정\n",
    "X_train = train.drop(columns=[TARGET, 'y_shifted'])\n",
    "y_train = train[['y_shifted']]\n",
    "\n",
    "# X_test, y_test 설정\n",
    "X_test = test.drop(columns=[TARGET, 'y_shifted'])\n",
    "y_test = test[['y_shifted']]\n",
    "\n",
    "# 결과 출력 (확인용)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19261, 68) (19261, 1) (696, 68) (696, 1)\n"
     ]
    }
   ],
   "source": [
    "# 넷 다 dataframe\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리 방식 정의\n",
    "fill_methods = {\n",
    "    '강수량(mm)': 0,\n",
    "    '풍속(m/s)': 'average',\n",
    "    '적설(cm)': 0\n",
    "}\n",
    "\n",
    "# 확장 가능한 결측값 처리 함수\n",
    "def fill_missing_expanding(df, column, max_offset=2400):\n",
    "    \"\"\"\n",
    "    결측값을 24, 48, 72, ..., max_offset 시간 뒤/전의 값으로 채우는 함수\n",
    "    \"\"\"\n",
    "    step = 24  # 24시간 간격\n",
    "    for offset in range(step, max_offset + step, step):  # 24, 48, 72, ..., max_offset\n",
    "        # 24시간 뒤와 전 값으로 채우기\n",
    "        df[column] = df[column].fillna(df[column].shift(offset))\n",
    "        df[column] = df[column].fillna(df[column].shift(-offset))\n",
    "        # 결측값이 모두 채워졌으면 반복 종료\n",
    "        if df[column].isna().sum() == 0:\n",
    "            break\n",
    "    # 남은 결측값이 있으면 ffill로 채우기\n",
    "    if df[column].isna().sum() != 0:\n",
    "        df[column] = df[column].ffill()\n",
    "    return df\n",
    "\n",
    "# 결측값 처리\n",
    "for df in [X_train, X_test]:\n",
    "    for column in df.columns:  # 모든 컬럼에 대해 처리\n",
    "        method = fill_methods.get(column, '24-48hour')  # 명시되지 않은 경우 기본값 '24-48hour'\n",
    "        \n",
    "        if method == '24-48hour':  # 24시간 뒤/전 방식으로 채우기\n",
    "            df = fill_missing_expanding(df, column)\n",
    "        elif method == 'average':  # 평균으로 채우기\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "        elif method == 'ffill':  # 이전 값으로 채우기\n",
    "            df[column] = df[column].ffill()\n",
    "        else:  # 특정 값으로 채우기\n",
    "            df[column] = df[column].fillna(method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 열을 float32로 변환\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050003300_velocity    0\n",
       "1050020400_velocity    0\n",
       "1070000200_velocity    0\n",
       "1070000500_velocity    0\n",
       "1070001600_velocity    0\n",
       "                      ..\n",
       "is_evening_rush        0\n",
       "month_sin              0\n",
       "month_cos              0\n",
       "hour_sin               0\n",
       "hour_cos               0\n",
       "Length: 68, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1050003300_velocity',\n",
       " '1050020400_velocity',\n",
       " '1070000200_velocity',\n",
       " '1070000500_velocity',\n",
       " '1070001600_velocity',\n",
       " '기온(°C)',\n",
       " '강수량(mm)',\n",
       " '풍속(m/s)',\n",
       " '습도(%)',\n",
       " '적설(cm)',\n",
       " 'A-12 유입_traffic',\n",
       " 'A-19 유입_traffic',\n",
       " 'A-20 유입_traffic',\n",
       " 'A-22 유입_traffic',\n",
       " 'A-22 유출_traffic',\n",
       " 'B-01 유입_traffic',\n",
       " 'B-06 유입_traffic',\n",
       " 'B-14 유입_traffic',\n",
       " 'B-14 유출_traffic',\n",
       " 'B-22 유입_traffic',\n",
       " 'B-22 유출_traffic',\n",
       " 'B-36 유출_traffic',\n",
       " 'C-02 유입_traffic',\n",
       " 'C-06 유입_traffic',\n",
       " 'C-07 유입_traffic',\n",
       " 'C-09 유출_traffic',\n",
       " 'C-17 유입_traffic',\n",
       " 'C-17 유출_traffic',\n",
       " 'C-20 유입_traffic',\n",
       " 'C-21 유출_traffic',\n",
       " 'D-04 유입_traffic',\n",
       " 'D-12 유출_traffic',\n",
       " 'D-16 유출_traffic',\n",
       " 'D-17 유입_traffic',\n",
       " 'D-17 유출_traffic',\n",
       " 'D-21 유출_traffic',\n",
       " 'D-28 유입_traffic',\n",
       " 'D-28 유출_traffic',\n",
       " 'D-31 유입_traffic',\n",
       " 'D-35 유입_traffic',\n",
       " 'D-35 유출_traffic',\n",
       " 'D-44 유출_traffic',\n",
       " 'F-02 유출_traffic',\n",
       " 'F-03 유입_traffic',\n",
       " 'F-03 유출_traffic',\n",
       " 'F-04 유출_traffic',\n",
       " 'F-05 유입_traffic',\n",
       " 'F-05 유출_traffic',\n",
       " 'F-06 유입_traffic',\n",
       " 'F-07 유입_traffic',\n",
       " 'F-07 유출_traffic',\n",
       " 'F-08 유입_traffic',\n",
       " 'F-08 유출_traffic',\n",
       " 'F-09 유입_traffic',\n",
       " 'F-09 유출_traffic',\n",
       " 'F-10 유입_traffic',\n",
       " 'F-10 유출_traffic',\n",
       " 'Year',\n",
       " 'is_weekend',\n",
       " 'is_holiday',\n",
       " 'is_morning_rush',\n",
       " 'is_evening_rush',\n",
       " 'month_sin',\n",
       " 'month_cos',\n",
       " 'hour_sin',\n",
       " 'hour_cos']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# X_train과 X_test를 하나로 합침\n",
    "X_combined = pd.concat([X_train, X_test], axis=0)\n",
    "\n",
    "# MinMaxScaler 적용\n",
    "scaler = MinMaxScaler()\n",
    "X_combined_scaled = pd.DataFrame(scaler.fit_transform(X_combined), columns=X_combined.columns)\n",
    "\n",
    "# 다시 분리\n",
    "X_train = X_combined_scaled.iloc[:len(X_train), :].reset_index(drop=True)\n",
    "X_test = X_combined_scaled.iloc[len(X_train):, :].reset_index(drop=True)\n",
    "\n",
    "X_train.drop([\"F-03 유입_traffic_x\", \"F-04 유출_traffic_x\"], axis=1, inplace=True)\n",
    "# 컬럼 이름 변경\n",
    "X_train.rename(columns={\"F-03 유입_traffic_y\": \"F-03 유입_traffic\"}, inplace=True)\n",
    "X_train.rename(columns={\"F-04 유출_traffic_y\": \"F-04 유출_traffic\"}, inplace=True)\n",
    "\n",
    "X_test.drop([\"F-03 유입_traffic_x\", \"F-04 유출_traffic_x\"], axis=1, inplace=True)\n",
    "# 컬럼 이름 변경\n",
    "X_test.rename(columns={\"F-03 유입_traffic_y\": \"F-03 유입_traffic\"}, inplace=True)\n",
    "X_test.rename(columns={\"F-04 유출_traffic_y\": \"F-04 유출_traffic\"}, inplace=True)\n",
    "\n",
    "X_train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "# Define the temporal and static features\n",
    "\n",
    "Train = pd.concat([X_train, y_train], axis=1)\n",
    "Target = \"y_shifted\"\n",
    "Train['time_idx'] = range(len(Train))\n",
    "Train['group_id'] = 0  # Single group for all Train\n",
    "\n",
    "Test = pd.concat([X_test, y_test], axis=1)\n",
    "Target = \"y_shifted\"\n",
    "Test['time_idx'] = range(len(Test))\n",
    "Test['group_id'] = 0  # Single group for all Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N_TEST = 12\n",
    "N_SPLIT = 27\n",
    "max_encoder_length = 24\n",
    "max_prediction_length = 1\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "Train.astype('float32')\n",
    "# Define features\n",
    "TEMPORAL_FEATURES = ['1050003300_velocity', '1050020400_velocity', '1070000200_velocity',\n",
    "                     '1070000500_velocity', '1070001600_velocity', \n",
    "                     'A-12 유입_traffic', 'A-19 유입_traffic', 'A-20 유입_traffic', 'A-22 유입_traffic', 'A-22 유출_traffic', 'B-01 유입_traffic', 'B-06 유입_traffic', 'B-14 유입_traffic', 'B-14 유출_traffic', 'B-22 유입_traffic', 'B-22 유출_traffic', 'B-36 유출_traffic', 'C-02 유입_traffic', 'C-06 유입_traffic', 'C-07 유입_traffic', 'C-09 유출_traffic', 'C-17 유입_traffic', 'C-17 유출_traffic', 'C-20 유입_traffic', 'C-21 유출_traffic', 'D-04 유입_traffic', 'D-12 유출_traffic', 'D-16 유출_traffic', 'D-17 유입_traffic', 'D-17 유출_traffic', 'D-21 유출_traffic', 'D-28 유입_traffic', 'D-28 유출_traffic', 'D-31 유입_traffic', 'D-35 유입_traffic', 'D-35 유출_traffic', 'D-44 유출_traffic', 'F-02 유출_traffic', 'F-03 유입_traffic', 'F-03 유출_traffic', 'F-04 유출_traffic', 'F-05 유입_traffic', 'F-05 유출_traffic', 'F-06 유입_traffic', 'F-07 유입_traffic', 'F-07 유출_traffic', 'F-08 유입_traffic', 'F-08 유출_traffic', 'F-09 유입_traffic', 'F-09 유출_traffic', 'F-10 유입_traffic', 'F-10 유출_traffic',\n",
    "                     '기온(°C)', '강수량(mm)', '풍속(m/s)', '습도(%)', '적설(cm)']\n",
    "STATIC_FEATURES = ['Year', 'is_weekend', 'is_holiday', 'is_morning_rush', 'is_evening_rush']\n",
    "\n",
    "# Fix the group_id and Year issues\n",
    "def preprocess_data(data):\n",
    "    data['group_id'] = 0\n",
    "    return data\n",
    "\n",
    "# Split and seed settings\n",
    "ts_split = TimeSeriesSplit(n_splits=N_SPLIT)\n",
    "mape_scores = []\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Temporal Fusion Transformer setup\n",
    "def setup_tft(Train, train_idx, val_idx):\n",
    "    Train = preprocess_data(Train)  # Preprocess the dataset\n",
    "    train_data = Train.iloc[train_idx]\n",
    "    val_data = Train.iloc[val_idx]\n",
    "    assert not train_data.isnull().any().any(), \"Missing values in train_data\"\n",
    "    assert not val_data.isnull().any().any(), \"Missing values in val_data\"\n",
    "\n",
    "    print(train_data.isnull().sum().sum())\n",
    "    train_dataset = TimeSeriesDataSet(\n",
    "        train_data,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"y_shifted\",\n",
    "        group_ids=[\"group_id\"],\n",
    "        min_encoder_length=max_encoder_length // 2,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_reals=STATIC_FEATURES,\n",
    "        time_varying_known_reals=TEMPORAL_FEATURES+[\"time_idx\"],\n",
    "        time_varying_unknown_reals=[\"y_shifted\"],\n",
    "        target_normalizer=None,\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "\n",
    "    val_dataset = TimeSeriesDataSet.from_dataset(\n",
    "        train_dataset, val_data\n",
    "    )\n",
    "    \n",
    "    # sample = train_dataset[0]\n",
    "    # print(sample)\n",
    "    #for key, value in sample.items():\n",
    "    #    print(f\"{key}: {value.shape if hasattr(value, 'shape') else value}\")\n",
    "\n",
    "    train_dataloader = train_dataset.to_dataloader(train=True, batch_size=128, num_workers=0)\n",
    "    val_dataloader = val_dataset.to_dataloader(train=False, batch_size=128 * 10, num_workers=0)\n",
    "    \n",
    "    from pytorch_forecasting.metrics import MAE\n",
    "    \n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        train_dataset,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        hidden_size=16,\n",
    "        attention_head_size=4,\n",
    "        dropout=0.1,\n",
    "        hidden_continuous_size=8,\n",
    "        output_size=1,\n",
    "        loss=MAE(),\n",
    "    )\n",
    "\n",
    "    return model, train_dataloader, val_dataloader\n",
    "\n",
    "# # Loop through splits\n",
    "# for fold, (train_idx, val_idx) in enumerate(ts_split.split(Train)):\n",
    "#     if fold < N_SPLIT - N_TEST:\n",
    "#         continue\n",
    "    \n",
    "#     print(f\"TFT Fold {fold + 1} start!\")\n",
    "#     import lightning.pytorch as lp\n",
    "#     # Train TFT\n",
    "#     tft_model, train_loader, val_loader = setup_tft(Train, train_idx, val_idx)\n",
    "    \n",
    "#     trainer = lp.Trainer(max_epochs=30, accelerator=\"cpu\")\n",
    "#     trainer.fit(tft_model, train_loader, val_loader)\n",
    "\n",
    "#     # Evaluate TFT\n",
    "#     tft_model.eval()\n",
    "\n",
    "#     actuals = torch.cat([y for x, y in iter(val_loader)]).cpu().numpy()\n",
    "#     predictions = torch.cat([tft_model.predict(x) for x, y in iter(val_loader)]).cpu().numpy()\n",
    "#     mape = mean_absolute_percentage_error(actuals, predictions)\n",
    "#     mape_scores.append(mape)\n",
    "#     print(f\"TFT Fold {fold + 1}, Validation MAPE: {mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "c:\\Users\\james\\anaconda3\\Lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py:171: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "c:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | MAE                             | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0      | train\n",
      "3  | prescalers                         | ModuleDict                      | 1.1 K  | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 42.2 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 41.5 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676    | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 17     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.406     Total estimated model params size (MB)\n",
      "2007      Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8c9c09928442fca48c2c0e6d7700a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomatic_optimization\u001b[38;5;241m.\u001b[39mrun(trainer\u001b[38;5;241m.\u001b[39moptimizers[\u001b[38;5;241m0\u001b[39m], batch_idx, kwargs)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step(batch_idx, closure)\n\u001b[0;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\n\u001b[0;32m    269\u001b[0m     trainer,\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_step\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    271\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mcurrent_epoch,\n\u001b[0;32m    272\u001b[0m     batch_idx,\n\u001b[0;32m    273\u001b[0m     optimizer,\n\u001b[0;32m    274\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \n\u001b[0;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1306\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:205\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 205\u001b[0m         loss \u001b[38;5;241m=\u001b[39m closure()\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m closure()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_fn()\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\pytorch_forecasting\\models\\base_model.py:653\u001b[0m, in \u001b[0;36mBaseModel.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    652\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m--> 653\u001b[0m log, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(x, y, batch_idx)\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step_outputs\u001b[38;5;241m.\u001b[39mappend(log)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\pytorch_forecasting\\models\\base_model.py:819\u001b[0m, in \u001b[0;36mBaseModel.step\u001b[1;34m(self, x, y, batch_idx, **kwargs)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 819\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py:463\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    460\u001b[0m embeddings_varying_encoder \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    461\u001b[0m     name: input_vectors[name][:, :max_encoder_length] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_variables\n\u001b[0;32m    462\u001b[0m }\n\u001b[1;32m--> 463\u001b[0m embeddings_varying_encoder, encoder_sparse_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_variable_selection(\n\u001b[0;32m    464\u001b[0m     embeddings_varying_encoder,\n\u001b[0;32m    465\u001b[0m     static_context_variable_selection[:, :max_encoder_length],\n\u001b[0;32m    466\u001b[0m )\n\u001b[0;32m    468\u001b[0m embeddings_varying_decoder \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    469\u001b[0m     name: input_vectors[name][:, max_encoder_length:] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_variables  \u001b[38;5;66;03m# select decoder\u001b[39;00m\n\u001b[0;32m    470\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\sub_modules.py:334\u001b[0m, in \u001b[0;36mVariableSelectionNetwork.forward\u001b[1;34m(self, x, context)\u001b[0m\n\u001b[0;32m    333\u001b[0m     weight_inputs\u001b[38;5;241m.\u001b[39mappend(variable_embedding)\n\u001b[1;32m--> 334\u001b[0m     var_outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingle_variable_grns[name](variable_embedding))\n\u001b[0;32m    335\u001b[0m var_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(var_outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\sub_modules.py:243\u001b[0m, in \u001b[0;36mGatedResidualNetwork.forward\u001b[1;34m(self, x, context, residual)\u001b[0m\n\u001b[0;32m    242\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m context\n\u001b[1;32m--> 243\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melu(x)\n\u001b[0;32m    244\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:532\u001b[0m, in \u001b[0;36mELU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1593\u001b[0m, in \u001b[0;36melu\u001b[1;34m(input, alpha, inplace)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1593\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28minput\u001b[39m, alpha)\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlp\u001b[39;00m\n\u001b[0;32m     43\u001b[0m trainer \u001b[38;5;241m=\u001b[39m lp\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(tft_model, train_dataloader)\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    540\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = TimeSeriesDataSet(\n",
    "    Train,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"y_shifted\",\n",
    "    group_ids=[\"group_id\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_reals=STATIC_FEATURES,\n",
    "    time_varying_known_reals=TEMPORAL_FEATURES + [\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\"y_shifted\"],\n",
    "    target_normalizer=None,\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "test_dataset = TimeSeriesDataSet.from_dataset(train_dataset, Test)\n",
    "\n",
    "# Dataloader setup\n",
    "train_dataloader = train_dataset.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "test_dataloader = test_dataset.to_dataloader(train=False, batch_size=BATCH_SIZE * 10, num_workers=0)\n",
    "\n",
    "# Model setup\n",
    "from pytorch_forecasting.metrics import MAE\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "\n",
    "tft_model = TemporalFusionTransformer.from_dataset(\n",
    "    train_dataset,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1,\n",
    "    loss=MAE(),\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "import lightning.pytorch as lp\n",
    "\n",
    "trainer = lp.Trainer(max_epochs=30, accelerator=\"cpu\")\n",
    "trainer.fit(tft_model, train_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\james\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions type: <class 'torch.Tensor'>\n",
      "Predictions shape: torch.Size([696, 1])\n",
      "Test MAPE: 0.0767\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "tft_model.eval()\n",
    "\n",
    "# 실제값 수집\n",
    "actuals = torch.cat([y[0] for x, y in iter(test_dataloader)]).cpu().numpy()\n",
    "\n",
    "# 예측값 수집\n",
    "predictions = tft_model.predict(test_dataloader, mode=\"prediction\")\n",
    "\n",
    "# 예측값 디버깅\n",
    "print(f\"Predictions type: {type(predictions)}\")\n",
    "print(f\"Predictions shape: {predictions.shape if isinstance(predictions, torch.Tensor) else [p.shape for p in predictions]}\")\n",
    "\n",
    "# 2차원 텐서로 변환\n",
    "if isinstance(predictions, list):\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "else:\n",
    "    predictions = predictions.cpu().numpy()\n",
    "# Calculate MAPE\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "test_mape = mean_absolute_percentage_error(actuals, predictions)\n",
    "print(f\"Test MAPE: {test_mape:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'encoder_cat': tensor([], size=(640, 24, 0), dtype=torch.int64), 'encoder_cont': tensor([[[ 1.4940, -0.6313, -0.2350,  ..., -1.7320, -1.0000, 33.1400],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7318, -0.9583, 35.3900],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7316, -0.9167, 36.0400],\n",
      "         ...,\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7282, -0.1250, 31.7100],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7280, -0.0833, 33.5500],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7278, -0.0417, 31.2400]],\n",
      "\n",
      "        [[ 1.4940, -0.6313, -0.2350,  ..., -1.7318, -1.0000, 35.3900],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7316, -0.9583, 36.0400],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7314, -0.9167, 39.1500],\n",
      "         ...,\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7280, -0.1250, 33.5500],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7278, -0.0833, 31.2400],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7276, -0.0417, 35.1700]],\n",
      "\n",
      "        [[ 1.4940, -0.6313, -0.2350,  ..., -1.7316, -1.0000, 36.0400],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7314, -0.9583, 39.1500],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7312, -0.9167, 37.1800],\n",
      "         ...,\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7278, -0.1250, 31.2400],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7276, -0.0833, 35.1700],\n",
      "         [ 1.4940, -0.6313, -0.2350,  ..., -1.7275, -0.0417, 34.6300]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4940,  1.5840, -0.2350,  ..., -1.6174, -1.0000, 29.4600],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6172, -0.9583, 28.8800],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6170, -0.9167, 27.0800],\n",
      "         ...,\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6136, -0.1250, 21.2400],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6134, -0.0833, 23.3300],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6133, -0.0417, 27.3400]],\n",
      "\n",
      "        [[ 1.4940,  1.5840, -0.2350,  ..., -1.6172, -1.0000, 28.8800],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6170, -0.9583, 27.0800],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6169, -0.9167, 32.4600],\n",
      "         ...,\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6134, -0.1250, 23.3300],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6133, -0.0833, 27.3400],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6131, -0.0417, 30.5100]],\n",
      "\n",
      "        [[ 1.4940,  1.5840, -0.2350,  ..., -1.6170, -1.0000, 27.0800],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6169, -0.9583, 32.4600],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6167, -0.9167, 32.4300],\n",
      "         ...,\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6133, -0.1250, 27.3400],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6131, -0.0833, 30.5100],\n",
      "         [ 1.4940,  1.5840, -0.2350,  ..., -1.6129, -0.0417, 28.3200]]]), 'encoder_target': tensor([[33.1400, 35.3900, 36.0400,  ..., 31.7100, 33.5500, 31.2400],\n",
      "        [35.3900, 36.0400, 39.1500,  ..., 33.5500, 31.2400, 35.1700],\n",
      "        [36.0400, 39.1500, 37.1800,  ..., 31.2400, 35.1700, 34.6300],\n",
      "        ...,\n",
      "        [29.4600, 28.8800, 27.0800,  ..., 21.2400, 23.3300, 27.3400],\n",
      "        [28.8800, 27.0800, 32.4600,  ..., 23.3300, 27.3400, 30.5100],\n",
      "        [27.0800, 32.4600, 32.4300,  ..., 27.3400, 30.5100, 28.3200]]), 'encoder_lengths': tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24]), 'decoder_cat': tensor([], size=(640, 1, 0), dtype=torch.int64), 'decoder_cont': tensor([[[ 1.4940, -0.6313, -0.2350,  ..., -1.7276,  0.0000, 35.1700]],\n",
      "\n",
      "        [[ 1.4940, -0.6313, -0.2350,  ..., -1.7275,  0.0000, 34.6300]],\n",
      "\n",
      "        [[ 1.4940, -0.6313, -0.2350,  ..., -1.7273,  0.0000, 33.9900]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4940,  1.5840, -0.2350,  ..., -1.6131,  0.0000, 30.5100]],\n",
      "\n",
      "        [[ 1.4940,  1.5840, -0.2350,  ..., -1.6129,  0.0000, 28.3200]],\n",
      "\n",
      "        [[ 1.4940,  1.5840, -0.2350,  ..., -1.6127,  0.0000, 23.7000]]]), 'decoder_target': tensor([[35.1700],\n",
      "        [34.6300],\n",
      "        [33.9900],\n",
      "        [39.3900],\n",
      "        [35.8200],\n",
      "        [32.2400],\n",
      "        [33.5000],\n",
      "        [29.5300],\n",
      "        [27.0700],\n",
      "        [25.7300],\n",
      "        [24.8900],\n",
      "        [28.6900],\n",
      "        [32.3700],\n",
      "        [30.2500],\n",
      "        [29.4000],\n",
      "        [31.6300],\n",
      "        [31.5700],\n",
      "        [30.5000],\n",
      "        [28.7200],\n",
      "        [29.0000],\n",
      "        [29.9400],\n",
      "        [30.3600],\n",
      "        [30.6500],\n",
      "        [33.9700],\n",
      "        [35.0500],\n",
      "        [35.6700],\n",
      "        [38.6100],\n",
      "        [35.8200],\n",
      "        [34.5800],\n",
      "        [33.0800],\n",
      "        [32.8500],\n",
      "        [28.8200],\n",
      "        [24.3200],\n",
      "        [24.9200],\n",
      "        [20.5700],\n",
      "        [25.7400],\n",
      "        [30.3600],\n",
      "        [28.6500],\n",
      "        [27.4300],\n",
      "        [29.1500],\n",
      "        [29.7500],\n",
      "        [30.3300],\n",
      "        [27.7500],\n",
      "        [26.8500],\n",
      "        [29.3800],\n",
      "        [31.5500],\n",
      "        [32.1000],\n",
      "        [31.7600],\n",
      "        [35.2500],\n",
      "        [36.7300],\n",
      "        [36.0500],\n",
      "        [37.4400],\n",
      "        [40.4200],\n",
      "        [34.9700],\n",
      "        [30.7000],\n",
      "        [28.6100],\n",
      "        [23.3200],\n",
      "        [22.1600],\n",
      "        [25.0400],\n",
      "        [29.0900],\n",
      "        [27.7300],\n",
      "        [28.7300],\n",
      "        [25.4600],\n",
      "        [25.0800],\n",
      "        [24.8200],\n",
      "        [23.3400],\n",
      "        [23.2400],\n",
      "        [32.5000],\n",
      "        [31.7400],\n",
      "        [32.6100],\n",
      "        [32.7500],\n",
      "        [34.1700],\n",
      "        [35.0500],\n",
      "        [35.8000],\n",
      "        [38.0100],\n",
      "        [38.5100],\n",
      "        [37.7000],\n",
      "        [36.4800],\n",
      "        [37.6200],\n",
      "        [36.5900],\n",
      "        [31.1900],\n",
      "        [30.3200],\n",
      "        [22.9900],\n",
      "        [20.4400],\n",
      "        [25.4000],\n",
      "        [27.7200],\n",
      "        [26.2700],\n",
      "        [28.0700],\n",
      "        [29.2900],\n",
      "        [27.2500],\n",
      "        [26.4100],\n",
      "        [29.1200],\n",
      "        [29.4500],\n",
      "        [31.4500],\n",
      "        [30.5500],\n",
      "        [34.6200],\n",
      "        [34.7700],\n",
      "        [33.1000],\n",
      "        [34.8100],\n",
      "        [40.2200],\n",
      "        [38.2700],\n",
      "        [41.9400],\n",
      "        [39.9300],\n",
      "        [38.2600],\n",
      "        [36.4700],\n",
      "        [33.1500],\n",
      "        [28.9800],\n",
      "        [28.9000],\n",
      "        [37.2300],\n",
      "        [30.6100],\n",
      "        [29.2300],\n",
      "        [28.7800],\n",
      "        [28.9900],\n",
      "        [29.7200],\n",
      "        [30.1300],\n",
      "        [27.7800],\n",
      "        [30.4400],\n",
      "        [31.2600],\n",
      "        [29.2600],\n",
      "        [32.8900],\n",
      "        [34.5200],\n",
      "        [36.9100],\n",
      "        [35.1400],\n",
      "        [36.2400],\n",
      "        [37.0900],\n",
      "        [35.5500],\n",
      "        [27.2600],\n",
      "        [27.3900],\n",
      "        [22.0300],\n",
      "        [22.4300],\n",
      "        [25.8600],\n",
      "        [30.3000],\n",
      "        [32.8500],\n",
      "        [28.7500],\n",
      "        [29.5400],\n",
      "        [29.3900],\n",
      "        [30.6600],\n",
      "        [29.4300],\n",
      "        [30.0300],\n",
      "        [28.1100],\n",
      "        [30.8000],\n",
      "        [34.2300],\n",
      "        [33.2600],\n",
      "        [33.0600],\n",
      "        [36.8500],\n",
      "        [33.5000],\n",
      "        [35.7700],\n",
      "        [39.7700],\n",
      "        [36.9600],\n",
      "        [33.8300],\n",
      "        [32.6500],\n",
      "        [28.8100],\n",
      "        [27.0100],\n",
      "        [24.7500],\n",
      "        [25.1100],\n",
      "        [29.1900],\n",
      "        [29.9300],\n",
      "        [29.0500],\n",
      "        [26.3500],\n",
      "        [27.3800],\n",
      "        [26.2500],\n",
      "        [25.8300],\n",
      "        [32.6000],\n",
      "        [29.3900],\n",
      "        [29.5500],\n",
      "        [32.7400],\n",
      "        [32.1900],\n",
      "        [31.3600],\n",
      "        [33.9900],\n",
      "        [34.6100],\n",
      "        [35.2100],\n",
      "        [34.9900],\n",
      "        [39.8100],\n",
      "        [37.0400],\n",
      "        [35.1600],\n",
      "        [30.0300],\n",
      "        [24.4800],\n",
      "        [23.7300],\n",
      "        [29.2800],\n",
      "        [28.7000],\n",
      "        [32.8800],\n",
      "        [30.4000],\n",
      "        [28.5200],\n",
      "        [26.5800],\n",
      "        [22.6900],\n",
      "        [19.3800],\n",
      "        [26.8200],\n",
      "        [30.0500],\n",
      "        [30.1600],\n",
      "        [29.6900],\n",
      "        [30.1900],\n",
      "        [33.2500],\n",
      "        [33.5100],\n",
      "        [35.1100],\n",
      "        [37.0100],\n",
      "        [39.5400],\n",
      "        [38.2100],\n",
      "        [35.0800],\n",
      "        [34.5900],\n",
      "        [27.7100],\n",
      "        [19.9600],\n",
      "        [23.4600],\n",
      "        [19.8200],\n",
      "        [22.1000],\n",
      "        [27.7200],\n",
      "        [27.3400],\n",
      "        [27.7700],\n",
      "        [28.0300],\n",
      "        [28.1200],\n",
      "        [21.4400],\n",
      "        [23.2300],\n",
      "        [27.0100],\n",
      "        [29.2600],\n",
      "        [30.0700],\n",
      "        [32.2300],\n",
      "        [32.8000],\n",
      "        [34.5400],\n",
      "        [34.2400],\n",
      "        [35.6400],\n",
      "        [38.2400],\n",
      "        [37.7700],\n",
      "        [36.5200],\n",
      "        [31.0000],\n",
      "        [29.1000],\n",
      "        [28.2800],\n",
      "        [28.7400],\n",
      "        [23.3300],\n",
      "        [22.7500],\n",
      "        [26.5700],\n",
      "        [26.5200],\n",
      "        [25.0900],\n",
      "        [26.1500],\n",
      "        [17.5400],\n",
      "        [11.9600],\n",
      "        [22.6400],\n",
      "        [24.3100],\n",
      "        [26.8200],\n",
      "        [32.1300],\n",
      "        [30.3300],\n",
      "        [32.9500],\n",
      "        [34.8500],\n",
      "        [35.2800],\n",
      "        [35.0800],\n",
      "        [35.1800],\n",
      "        [38.7500],\n",
      "        [38.8400],\n",
      "        [35.6200],\n",
      "        [30.9300],\n",
      "        [31.6100],\n",
      "        [33.1200],\n",
      "        [24.5300],\n",
      "        [18.9200],\n",
      "        [21.1800],\n",
      "        [24.9600],\n",
      "        [24.9000],\n",
      "        [23.6700],\n",
      "        [21.3700],\n",
      "        [20.9400],\n",
      "        [22.6400],\n",
      "        [30.0000],\n",
      "        [31.3200],\n",
      "        [30.3800],\n",
      "        [33.1700],\n",
      "        [34.5700],\n",
      "        [35.4100],\n",
      "        [35.1800],\n",
      "        [36.1100],\n",
      "        [37.4800],\n",
      "        [38.7100],\n",
      "        [34.1800],\n",
      "        [35.9700],\n",
      "        [35.9900],\n",
      "        [37.2800],\n",
      "        [34.4700],\n",
      "        [29.9700],\n",
      "        [27.1700],\n",
      "        [30.1400],\n",
      "        [30.3300],\n",
      "        [29.5600],\n",
      "        [30.0400],\n",
      "        [29.2100],\n",
      "        [28.8100],\n",
      "        [28.9200],\n",
      "        [29.4100],\n",
      "        [28.6400],\n",
      "        [28.9400],\n",
      "        [30.4500],\n",
      "        [33.9300],\n",
      "        [35.8100],\n",
      "        [36.8600],\n",
      "        [42.4500],\n",
      "        [37.2900],\n",
      "        [37.0200],\n",
      "        [36.5900],\n",
      "        [31.3200],\n",
      "        [28.4500],\n",
      "        [26.6500],\n",
      "        [22.5500],\n",
      "        [20.6500],\n",
      "        [24.8800],\n",
      "        [29.4400],\n",
      "        [27.7600],\n",
      "        [29.4700],\n",
      "        [34.0800],\n",
      "        [26.2700],\n",
      "        [19.2300],\n",
      "        [20.9000],\n",
      "        [28.8200],\n",
      "        [31.6900],\n",
      "        [31.2300],\n",
      "        [29.9600],\n",
      "        [34.4500],\n",
      "        [33.6800],\n",
      "        [34.5900],\n",
      "        [37.9300],\n",
      "        [36.9900],\n",
      "        [36.3600],\n",
      "        [33.8200],\n",
      "        [31.8800],\n",
      "        [30.1700],\n",
      "        [28.2100],\n",
      "        [23.9200],\n",
      "        [24.4300],\n",
      "        [24.8700],\n",
      "        [30.2800],\n",
      "        [28.1400],\n",
      "        [28.7400],\n",
      "        [28.9500],\n",
      "        [16.7600],\n",
      "        [25.8600],\n",
      "        [29.3600],\n",
      "        [26.8500],\n",
      "        [27.4300],\n",
      "        [32.4000],\n",
      "        [31.3500],\n",
      "        [30.8900],\n",
      "        [34.6300],\n",
      "        [36.4300],\n",
      "        [37.9100],\n",
      "        [37.3100],\n",
      "        [36.6600],\n",
      "        [35.0300],\n",
      "        [31.3800],\n",
      "        [23.9600],\n",
      "        [25.7000],\n",
      "        [18.5800],\n",
      "        [16.4100],\n",
      "        [16.8900],\n",
      "        [18.9400],\n",
      "        [22.5900],\n",
      "        [29.5600],\n",
      "        [29.1300],\n",
      "        [25.8500],\n",
      "        [21.7800],\n",
      "        [27.9000],\n",
      "        [25.6300],\n",
      "        [29.6600],\n",
      "        [31.5600],\n",
      "        [31.8700],\n",
      "        [32.4100],\n",
      "        [31.9700],\n",
      "        [36.0500],\n",
      "        [39.2500],\n",
      "        [36.2400],\n",
      "        [32.7200],\n",
      "        [30.5800],\n",
      "        [24.0700],\n",
      "        [17.6500],\n",
      "        [33.6700],\n",
      "        [17.8400],\n",
      "        [15.6100],\n",
      "        [14.2100],\n",
      "        [14.3000],\n",
      "        [19.5400],\n",
      "        [18.5800],\n",
      "        [20.1400],\n",
      "        [18.5100],\n",
      "        [20.1700],\n",
      "        [25.8400],\n",
      "        [32.2900],\n",
      "        [30.5500],\n",
      "        [31.5900],\n",
      "        [30.9100],\n",
      "        [32.4700],\n",
      "        [36.1500],\n",
      "        [37.8800],\n",
      "        [37.5900],\n",
      "        [40.8400],\n",
      "        [38.0200],\n",
      "        [38.1900],\n",
      "        [33.8800],\n",
      "        [29.3700],\n",
      "        [27.6900],\n",
      "        [26.5600],\n",
      "        [26.2800],\n",
      "        [24.6900],\n",
      "        [30.4500],\n",
      "        [29.6800],\n",
      "        [27.0700],\n",
      "        [20.3800],\n",
      "        [12.5800],\n",
      "        [21.3000],\n",
      "        [24.5200],\n",
      "        [27.4400],\n",
      "        [29.7800],\n",
      "        [31.1000],\n",
      "        [33.1500],\n",
      "        [36.3000],\n",
      "        [34.2900],\n",
      "        [35.5900],\n",
      "        [36.8900],\n",
      "        [37.4400],\n",
      "        [36.8900],\n",
      "        [37.3100],\n",
      "        [37.6100],\n",
      "        [33.9000],\n",
      "        [35.7800],\n",
      "        [28.6300],\n",
      "        [23.4500],\n",
      "        [22.7000],\n",
      "        [25.1200],\n",
      "        [25.3400],\n",
      "        [29.1700],\n",
      "        [28.2000],\n",
      "        [24.8000],\n",
      "        [19.6100],\n",
      "        [22.6900],\n",
      "        [25.8100],\n",
      "        [30.0500],\n",
      "        [31.6300],\n",
      "        [31.4100],\n",
      "        [34.6800],\n",
      "        [34.6800],\n",
      "        [36.6100],\n",
      "        [37.0900],\n",
      "        [36.3500],\n",
      "        [36.7200],\n",
      "        [35.9200],\n",
      "        [35.3300],\n",
      "        [35.5000],\n",
      "        [36.5600],\n",
      "        [32.3900],\n",
      "        [30.6300],\n",
      "        [30.6500],\n",
      "        [30.8800],\n",
      "        [30.5800],\n",
      "        [29.4600],\n",
      "        [31.0100],\n",
      "        [32.1100],\n",
      "        [29.3300],\n",
      "        [35.1600],\n",
      "        [30.8100],\n",
      "        [33.0100],\n",
      "        [33.3800],\n",
      "        [33.8900],\n",
      "        [32.6200],\n",
      "        [35.1600],\n",
      "        [38.9600],\n",
      "        [39.2500],\n",
      "        [38.2600],\n",
      "        [38.2200],\n",
      "        [34.9200],\n",
      "        [32.3800],\n",
      "        [30.4000],\n",
      "        [27.0200],\n",
      "        [23.9000],\n",
      "        [26.0300],\n",
      "        [27.8900],\n",
      "        [30.5000],\n",
      "        [29.4800],\n",
      "        [29.3700],\n",
      "        [25.9200],\n",
      "        [26.8200],\n",
      "        [28.8900],\n",
      "        [29.4700],\n",
      "        [27.8700],\n",
      "        [28.9100],\n",
      "        [31.9500],\n",
      "        [32.0000],\n",
      "        [30.7400],\n",
      "        [33.1500],\n",
      "        [37.4300],\n",
      "        [38.2300],\n",
      "        [36.9800],\n",
      "        [38.2800],\n",
      "        [37.3500],\n",
      "        [32.8300],\n",
      "        [30.7800],\n",
      "        [27.3600],\n",
      "        [28.5000],\n",
      "        [26.3500],\n",
      "        [26.9400],\n",
      "        [26.7700],\n",
      "        [28.2200],\n",
      "        [27.4300],\n",
      "        [29.8300],\n",
      "        [26.6900],\n",
      "        [31.2500],\n",
      "        [30.4400],\n",
      "        [31.7300],\n",
      "        [32.4700],\n",
      "        [30.3300],\n",
      "        [30.1500],\n",
      "        [33.5800],\n",
      "        [34.1500],\n",
      "        [34.7400],\n",
      "        [35.8800],\n",
      "        [40.2800],\n",
      "        [37.7400],\n",
      "        [35.3400],\n",
      "        [33.3000],\n",
      "        [28.6000],\n",
      "        [23.2300],\n",
      "        [24.3800],\n",
      "        [22.9200],\n",
      "        [23.8700],\n",
      "        [31.1500],\n",
      "        [29.9700],\n",
      "        [29.3000],\n",
      "        [27.6100],\n",
      "        [27.0500],\n",
      "        [22.2500],\n",
      "        [25.7300],\n",
      "        [28.4000],\n",
      "        [30.1600],\n",
      "        [30.8200],\n",
      "        [31.7700],\n",
      "        [32.0200],\n",
      "        [34.6200],\n",
      "        [34.0500],\n",
      "        [35.1300],\n",
      "        [40.2300],\n",
      "        [38.2000],\n",
      "        [35.2800],\n",
      "        [32.0500],\n",
      "        [25.5600],\n",
      "        [25.9600],\n",
      "        [27.2900],\n",
      "        [20.7000],\n",
      "        [20.9100],\n",
      "        [24.7200],\n",
      "        [24.5100],\n",
      "        [27.3000],\n",
      "        [29.9300],\n",
      "        [23.6500],\n",
      "        [12.4600],\n",
      "        [19.7100],\n",
      "        [24.9700],\n",
      "        [28.9500],\n",
      "        [27.3900],\n",
      "        [29.2300],\n",
      "        [32.5600],\n",
      "        [33.2100],\n",
      "        [34.7600],\n",
      "        [36.7800],\n",
      "        [36.5800],\n",
      "        [39.2400],\n",
      "        [36.3700],\n",
      "        [34.0100],\n",
      "        [29.7500],\n",
      "        [25.6200],\n",
      "        [25.1400],\n",
      "        [21.8500],\n",
      "        [24.9700],\n",
      "        [28.0900],\n",
      "        [25.7600],\n",
      "        [24.0400],\n",
      "        [26.8800],\n",
      "        [16.8600],\n",
      "        [11.7400],\n",
      "        [15.8300],\n",
      "        [21.9800],\n",
      "        [25.2300],\n",
      "        [31.0500],\n",
      "        [31.1700],\n",
      "        [32.1800],\n",
      "        [36.6000],\n",
      "        [35.8300],\n",
      "        [37.8100],\n",
      "        [36.1600],\n",
      "        [37.9700],\n",
      "        [36.7100],\n",
      "        [36.0400],\n",
      "        [35.5200],\n",
      "        [31.5400],\n",
      "        [28.5300],\n",
      "        [29.8100],\n",
      "        [26.9600],\n",
      "        [25.7300],\n",
      "        [25.3800],\n",
      "        [23.4300],\n",
      "        [19.9000],\n",
      "        [22.6500],\n",
      "        [21.5900],\n",
      "        [16.9800],\n",
      "        [26.2100],\n",
      "        [31.1600],\n",
      "        [31.6700],\n",
      "        [31.6700],\n",
      "        [33.7300],\n",
      "        [35.0200],\n",
      "        [35.4200],\n",
      "        [38.7000],\n",
      "        [38.5400],\n",
      "        [37.9500],\n",
      "        [38.5800],\n",
      "        [37.5300],\n",
      "        [38.9200],\n",
      "        [34.6500],\n",
      "        [32.7000],\n",
      "        [28.7800],\n",
      "        [29.8000],\n",
      "        [31.2200],\n",
      "        [29.4600],\n",
      "        [28.8800],\n",
      "        [27.0800],\n",
      "        [32.4600],\n",
      "        [32.4300],\n",
      "        [32.2300],\n",
      "        [27.5300],\n",
      "        [27.3700],\n",
      "        [30.8100],\n",
      "        [31.7900],\n",
      "        [31.6700],\n",
      "        [33.5000],\n",
      "        [34.9000],\n",
      "        [34.7200],\n",
      "        [38.7600],\n",
      "        [38.1500],\n",
      "        [36.5200],\n",
      "        [31.2900],\n",
      "        [26.6300],\n",
      "        [25.3700],\n",
      "        [25.2600],\n",
      "        [21.2400],\n",
      "        [23.3300],\n",
      "        [27.3400],\n",
      "        [30.5100],\n",
      "        [28.3200],\n",
      "        [23.7000]]), 'decoder_lengths': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'decoder_time_idx': tensor([[ 24],\n",
      "        [ 25],\n",
      "        [ 26],\n",
      "        [ 27],\n",
      "        [ 28],\n",
      "        [ 29],\n",
      "        [ 30],\n",
      "        [ 31],\n",
      "        [ 32],\n",
      "        [ 33],\n",
      "        [ 34],\n",
      "        [ 35],\n",
      "        [ 36],\n",
      "        [ 37],\n",
      "        [ 38],\n",
      "        [ 39],\n",
      "        [ 40],\n",
      "        [ 41],\n",
      "        [ 42],\n",
      "        [ 43],\n",
      "        [ 44],\n",
      "        [ 45],\n",
      "        [ 46],\n",
      "        [ 47],\n",
      "        [ 48],\n",
      "        [ 49],\n",
      "        [ 50],\n",
      "        [ 51],\n",
      "        [ 52],\n",
      "        [ 53],\n",
      "        [ 54],\n",
      "        [ 55],\n",
      "        [ 56],\n",
      "        [ 57],\n",
      "        [ 58],\n",
      "        [ 59],\n",
      "        [ 60],\n",
      "        [ 61],\n",
      "        [ 62],\n",
      "        [ 63],\n",
      "        [ 64],\n",
      "        [ 65],\n",
      "        [ 66],\n",
      "        [ 67],\n",
      "        [ 68],\n",
      "        [ 69],\n",
      "        [ 70],\n",
      "        [ 71],\n",
      "        [ 72],\n",
      "        [ 73],\n",
      "        [ 74],\n",
      "        [ 75],\n",
      "        [ 76],\n",
      "        [ 77],\n",
      "        [ 78],\n",
      "        [ 79],\n",
      "        [ 80],\n",
      "        [ 81],\n",
      "        [ 82],\n",
      "        [ 83],\n",
      "        [ 84],\n",
      "        [ 85],\n",
      "        [ 86],\n",
      "        [ 87],\n",
      "        [ 88],\n",
      "        [ 89],\n",
      "        [ 90],\n",
      "        [ 91],\n",
      "        [ 92],\n",
      "        [ 93],\n",
      "        [ 94],\n",
      "        [ 95],\n",
      "        [ 96],\n",
      "        [ 97],\n",
      "        [ 98],\n",
      "        [ 99],\n",
      "        [100],\n",
      "        [101],\n",
      "        [102],\n",
      "        [103],\n",
      "        [104],\n",
      "        [105],\n",
      "        [106],\n",
      "        [107],\n",
      "        [108],\n",
      "        [109],\n",
      "        [110],\n",
      "        [111],\n",
      "        [112],\n",
      "        [113],\n",
      "        [114],\n",
      "        [115],\n",
      "        [116],\n",
      "        [117],\n",
      "        [118],\n",
      "        [119],\n",
      "        [120],\n",
      "        [121],\n",
      "        [122],\n",
      "        [123],\n",
      "        [124],\n",
      "        [125],\n",
      "        [126],\n",
      "        [127],\n",
      "        [128],\n",
      "        [129],\n",
      "        [130],\n",
      "        [131],\n",
      "        [132],\n",
      "        [133],\n",
      "        [134],\n",
      "        [135],\n",
      "        [136],\n",
      "        [137],\n",
      "        [138],\n",
      "        [139],\n",
      "        [140],\n",
      "        [141],\n",
      "        [142],\n",
      "        [143],\n",
      "        [144],\n",
      "        [145],\n",
      "        [146],\n",
      "        [147],\n",
      "        [148],\n",
      "        [149],\n",
      "        [150],\n",
      "        [151],\n",
      "        [152],\n",
      "        [153],\n",
      "        [154],\n",
      "        [155],\n",
      "        [156],\n",
      "        [157],\n",
      "        [158],\n",
      "        [159],\n",
      "        [160],\n",
      "        [161],\n",
      "        [162],\n",
      "        [163],\n",
      "        [164],\n",
      "        [165],\n",
      "        [166],\n",
      "        [167],\n",
      "        [168],\n",
      "        [169],\n",
      "        [170],\n",
      "        [171],\n",
      "        [172],\n",
      "        [173],\n",
      "        [174],\n",
      "        [175],\n",
      "        [176],\n",
      "        [177],\n",
      "        [178],\n",
      "        [179],\n",
      "        [180],\n",
      "        [181],\n",
      "        [182],\n",
      "        [183],\n",
      "        [184],\n",
      "        [185],\n",
      "        [186],\n",
      "        [187],\n",
      "        [188],\n",
      "        [189],\n",
      "        [190],\n",
      "        [191],\n",
      "        [192],\n",
      "        [193],\n",
      "        [194],\n",
      "        [195],\n",
      "        [196],\n",
      "        [197],\n",
      "        [198],\n",
      "        [199],\n",
      "        [200],\n",
      "        [201],\n",
      "        [202],\n",
      "        [203],\n",
      "        [204],\n",
      "        [205],\n",
      "        [206],\n",
      "        [207],\n",
      "        [208],\n",
      "        [209],\n",
      "        [210],\n",
      "        [211],\n",
      "        [212],\n",
      "        [213],\n",
      "        [214],\n",
      "        [215],\n",
      "        [216],\n",
      "        [217],\n",
      "        [218],\n",
      "        [219],\n",
      "        [220],\n",
      "        [221],\n",
      "        [222],\n",
      "        [223],\n",
      "        [224],\n",
      "        [225],\n",
      "        [226],\n",
      "        [227],\n",
      "        [228],\n",
      "        [229],\n",
      "        [230],\n",
      "        [231],\n",
      "        [232],\n",
      "        [233],\n",
      "        [234],\n",
      "        [235],\n",
      "        [236],\n",
      "        [237],\n",
      "        [238],\n",
      "        [239],\n",
      "        [240],\n",
      "        [241],\n",
      "        [242],\n",
      "        [243],\n",
      "        [244],\n",
      "        [245],\n",
      "        [246],\n",
      "        [247],\n",
      "        [248],\n",
      "        [249],\n",
      "        [250],\n",
      "        [251],\n",
      "        [252],\n",
      "        [253],\n",
      "        [254],\n",
      "        [255],\n",
      "        [256],\n",
      "        [257],\n",
      "        [258],\n",
      "        [259],\n",
      "        [260],\n",
      "        [261],\n",
      "        [262],\n",
      "        [263],\n",
      "        [264],\n",
      "        [265],\n",
      "        [266],\n",
      "        [267],\n",
      "        [268],\n",
      "        [269],\n",
      "        [270],\n",
      "        [271],\n",
      "        [272],\n",
      "        [273],\n",
      "        [274],\n",
      "        [275],\n",
      "        [276],\n",
      "        [277],\n",
      "        [278],\n",
      "        [279],\n",
      "        [280],\n",
      "        [281],\n",
      "        [282],\n",
      "        [283],\n",
      "        [284],\n",
      "        [285],\n",
      "        [286],\n",
      "        [287],\n",
      "        [288],\n",
      "        [289],\n",
      "        [290],\n",
      "        [291],\n",
      "        [292],\n",
      "        [293],\n",
      "        [294],\n",
      "        [295],\n",
      "        [296],\n",
      "        [297],\n",
      "        [298],\n",
      "        [299],\n",
      "        [300],\n",
      "        [301],\n",
      "        [302],\n",
      "        [303],\n",
      "        [304],\n",
      "        [305],\n",
      "        [306],\n",
      "        [307],\n",
      "        [308],\n",
      "        [309],\n",
      "        [310],\n",
      "        [311],\n",
      "        [312],\n",
      "        [313],\n",
      "        [314],\n",
      "        [315],\n",
      "        [316],\n",
      "        [317],\n",
      "        [318],\n",
      "        [319],\n",
      "        [320],\n",
      "        [321],\n",
      "        [322],\n",
      "        [323],\n",
      "        [324],\n",
      "        [325],\n",
      "        [326],\n",
      "        [327],\n",
      "        [328],\n",
      "        [329],\n",
      "        [330],\n",
      "        [331],\n",
      "        [332],\n",
      "        [333],\n",
      "        [334],\n",
      "        [335],\n",
      "        [336],\n",
      "        [337],\n",
      "        [338],\n",
      "        [339],\n",
      "        [340],\n",
      "        [341],\n",
      "        [342],\n",
      "        [343],\n",
      "        [344],\n",
      "        [345],\n",
      "        [346],\n",
      "        [347],\n",
      "        [348],\n",
      "        [349],\n",
      "        [350],\n",
      "        [351],\n",
      "        [352],\n",
      "        [353],\n",
      "        [354],\n",
      "        [355],\n",
      "        [356],\n",
      "        [357],\n",
      "        [358],\n",
      "        [359],\n",
      "        [360],\n",
      "        [361],\n",
      "        [362],\n",
      "        [363],\n",
      "        [364],\n",
      "        [365],\n",
      "        [366],\n",
      "        [367],\n",
      "        [368],\n",
      "        [369],\n",
      "        [370],\n",
      "        [371],\n",
      "        [372],\n",
      "        [373],\n",
      "        [374],\n",
      "        [375],\n",
      "        [376],\n",
      "        [377],\n",
      "        [378],\n",
      "        [379],\n",
      "        [380],\n",
      "        [381],\n",
      "        [382],\n",
      "        [383],\n",
      "        [384],\n",
      "        [385],\n",
      "        [386],\n",
      "        [387],\n",
      "        [388],\n",
      "        [389],\n",
      "        [390],\n",
      "        [391],\n",
      "        [392],\n",
      "        [393],\n",
      "        [394],\n",
      "        [395],\n",
      "        [396],\n",
      "        [397],\n",
      "        [398],\n",
      "        [399],\n",
      "        [400],\n",
      "        [401],\n",
      "        [402],\n",
      "        [403],\n",
      "        [404],\n",
      "        [405],\n",
      "        [406],\n",
      "        [407],\n",
      "        [408],\n",
      "        [409],\n",
      "        [410],\n",
      "        [411],\n",
      "        [412],\n",
      "        [413],\n",
      "        [414],\n",
      "        [415],\n",
      "        [416],\n",
      "        [417],\n",
      "        [418],\n",
      "        [419],\n",
      "        [420],\n",
      "        [421],\n",
      "        [422],\n",
      "        [423],\n",
      "        [424],\n",
      "        [425],\n",
      "        [426],\n",
      "        [427],\n",
      "        [428],\n",
      "        [429],\n",
      "        [430],\n",
      "        [431],\n",
      "        [432],\n",
      "        [433],\n",
      "        [434],\n",
      "        [435],\n",
      "        [436],\n",
      "        [437],\n",
      "        [438],\n",
      "        [439],\n",
      "        [440],\n",
      "        [441],\n",
      "        [442],\n",
      "        [443],\n",
      "        [444],\n",
      "        [445],\n",
      "        [446],\n",
      "        [447],\n",
      "        [448],\n",
      "        [449],\n",
      "        [450],\n",
      "        [451],\n",
      "        [452],\n",
      "        [453],\n",
      "        [454],\n",
      "        [455],\n",
      "        [456],\n",
      "        [457],\n",
      "        [458],\n",
      "        [459],\n",
      "        [460],\n",
      "        [461],\n",
      "        [462],\n",
      "        [463],\n",
      "        [464],\n",
      "        [465],\n",
      "        [466],\n",
      "        [467],\n",
      "        [468],\n",
      "        [469],\n",
      "        [470],\n",
      "        [471],\n",
      "        [472],\n",
      "        [473],\n",
      "        [474],\n",
      "        [475],\n",
      "        [476],\n",
      "        [477],\n",
      "        [478],\n",
      "        [479],\n",
      "        [480],\n",
      "        [481],\n",
      "        [482],\n",
      "        [483],\n",
      "        [484],\n",
      "        [485],\n",
      "        [486],\n",
      "        [487],\n",
      "        [488],\n",
      "        [489],\n",
      "        [490],\n",
      "        [491],\n",
      "        [492],\n",
      "        [493],\n",
      "        [494],\n",
      "        [495],\n",
      "        [496],\n",
      "        [497],\n",
      "        [498],\n",
      "        [499],\n",
      "        [500],\n",
      "        [501],\n",
      "        [502],\n",
      "        [503],\n",
      "        [504],\n",
      "        [505],\n",
      "        [506],\n",
      "        [507],\n",
      "        [508],\n",
      "        [509],\n",
      "        [510],\n",
      "        [511],\n",
      "        [512],\n",
      "        [513],\n",
      "        [514],\n",
      "        [515],\n",
      "        [516],\n",
      "        [517],\n",
      "        [518],\n",
      "        [519],\n",
      "        [520],\n",
      "        [521],\n",
      "        [522],\n",
      "        [523],\n",
      "        [524],\n",
      "        [525],\n",
      "        [526],\n",
      "        [527],\n",
      "        [528],\n",
      "        [529],\n",
      "        [530],\n",
      "        [531],\n",
      "        [532],\n",
      "        [533],\n",
      "        [534],\n",
      "        [535],\n",
      "        [536],\n",
      "        [537],\n",
      "        [538],\n",
      "        [539],\n",
      "        [540],\n",
      "        [541],\n",
      "        [542],\n",
      "        [543],\n",
      "        [544],\n",
      "        [545],\n",
      "        [546],\n",
      "        [547],\n",
      "        [548],\n",
      "        [549],\n",
      "        [550],\n",
      "        [551],\n",
      "        [552],\n",
      "        [553],\n",
      "        [554],\n",
      "        [555],\n",
      "        [556],\n",
      "        [557],\n",
      "        [558],\n",
      "        [559],\n",
      "        [560],\n",
      "        [561],\n",
      "        [562],\n",
      "        [563],\n",
      "        [564],\n",
      "        [565],\n",
      "        [566],\n",
      "        [567],\n",
      "        [568],\n",
      "        [569],\n",
      "        [570],\n",
      "        [571],\n",
      "        [572],\n",
      "        [573],\n",
      "        [574],\n",
      "        [575],\n",
      "        [576],\n",
      "        [577],\n",
      "        [578],\n",
      "        [579],\n",
      "        [580],\n",
      "        [581],\n",
      "        [582],\n",
      "        [583],\n",
      "        [584],\n",
      "        [585],\n",
      "        [586],\n",
      "        [587],\n",
      "        [588],\n",
      "        [589],\n",
      "        [590],\n",
      "        [591],\n",
      "        [592],\n",
      "        [593],\n",
      "        [594],\n",
      "        [595],\n",
      "        [596],\n",
      "        [597],\n",
      "        [598],\n",
      "        [599],\n",
      "        [600],\n",
      "        [601],\n",
      "        [602],\n",
      "        [603],\n",
      "        [604],\n",
      "        [605],\n",
      "        [606],\n",
      "        [607],\n",
      "        [608],\n",
      "        [609],\n",
      "        [610],\n",
      "        [611],\n",
      "        [612],\n",
      "        [613],\n",
      "        [614],\n",
      "        [615],\n",
      "        [616],\n",
      "        [617],\n",
      "        [618],\n",
      "        [619],\n",
      "        [620],\n",
      "        [621],\n",
      "        [622],\n",
      "        [623],\n",
      "        [624],\n",
      "        [625],\n",
      "        [626],\n",
      "        [627],\n",
      "        [628],\n",
      "        [629],\n",
      "        [630],\n",
      "        [631],\n",
      "        [632],\n",
      "        [633],\n",
      "        [634],\n",
      "        [635],\n",
      "        [636],\n",
      "        [637],\n",
      "        [638],\n",
      "        [639],\n",
      "        [640],\n",
      "        [641],\n",
      "        [642],\n",
      "        [643],\n",
      "        [644],\n",
      "        [645],\n",
      "        [646],\n",
      "        [647],\n",
      "        [648],\n",
      "        [649],\n",
      "        [650],\n",
      "        [651],\n",
      "        [652],\n",
      "        [653],\n",
      "        [654],\n",
      "        [655],\n",
      "        [656],\n",
      "        [657],\n",
      "        [658],\n",
      "        [659],\n",
      "        [660],\n",
      "        [661],\n",
      "        [662],\n",
      "        [663]]), 'groups': tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]), 'target_scale': tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        ...,\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])}, (tensor([[35.1700],\n",
      "        [34.6300],\n",
      "        [33.9900],\n",
      "        [39.3900],\n",
      "        [35.8200],\n",
      "        [32.2400],\n",
      "        [33.5000],\n",
      "        [29.5300],\n",
      "        [27.0700],\n",
      "        [25.7300],\n",
      "        [24.8900],\n",
      "        [28.6900],\n",
      "        [32.3700],\n",
      "        [30.2500],\n",
      "        [29.4000],\n",
      "        [31.6300],\n",
      "        [31.5700],\n",
      "        [30.5000],\n",
      "        [28.7200],\n",
      "        [29.0000],\n",
      "        [29.9400],\n",
      "        [30.3600],\n",
      "        [30.6500],\n",
      "        [33.9700],\n",
      "        [35.0500],\n",
      "        [35.6700],\n",
      "        [38.6100],\n",
      "        [35.8200],\n",
      "        [34.5800],\n",
      "        [33.0800],\n",
      "        [32.8500],\n",
      "        [28.8200],\n",
      "        [24.3200],\n",
      "        [24.9200],\n",
      "        [20.5700],\n",
      "        [25.7400],\n",
      "        [30.3600],\n",
      "        [28.6500],\n",
      "        [27.4300],\n",
      "        [29.1500],\n",
      "        [29.7500],\n",
      "        [30.3300],\n",
      "        [27.7500],\n",
      "        [26.8500],\n",
      "        [29.3800],\n",
      "        [31.5500],\n",
      "        [32.1000],\n",
      "        [31.7600],\n",
      "        [35.2500],\n",
      "        [36.7300],\n",
      "        [36.0500],\n",
      "        [37.4400],\n",
      "        [40.4200],\n",
      "        [34.9700],\n",
      "        [30.7000],\n",
      "        [28.6100],\n",
      "        [23.3200],\n",
      "        [22.1600],\n",
      "        [25.0400],\n",
      "        [29.0900],\n",
      "        [27.7300],\n",
      "        [28.7300],\n",
      "        [25.4600],\n",
      "        [25.0800],\n",
      "        [24.8200],\n",
      "        [23.3400],\n",
      "        [23.2400],\n",
      "        [32.5000],\n",
      "        [31.7400],\n",
      "        [32.6100],\n",
      "        [32.7500],\n",
      "        [34.1700],\n",
      "        [35.0500],\n",
      "        [35.8000],\n",
      "        [38.0100],\n",
      "        [38.5100],\n",
      "        [37.7000],\n",
      "        [36.4800],\n",
      "        [37.6200],\n",
      "        [36.5900],\n",
      "        [31.1900],\n",
      "        [30.3200],\n",
      "        [22.9900],\n",
      "        [20.4400],\n",
      "        [25.4000],\n",
      "        [27.7200],\n",
      "        [26.2700],\n",
      "        [28.0700],\n",
      "        [29.2900],\n",
      "        [27.2500],\n",
      "        [26.4100],\n",
      "        [29.1200],\n",
      "        [29.4500],\n",
      "        [31.4500],\n",
      "        [30.5500],\n",
      "        [34.6200],\n",
      "        [34.7700],\n",
      "        [33.1000],\n",
      "        [34.8100],\n",
      "        [40.2200],\n",
      "        [38.2700],\n",
      "        [41.9400],\n",
      "        [39.9300],\n",
      "        [38.2600],\n",
      "        [36.4700],\n",
      "        [33.1500],\n",
      "        [28.9800],\n",
      "        [28.9000],\n",
      "        [37.2300],\n",
      "        [30.6100],\n",
      "        [29.2300],\n",
      "        [28.7800],\n",
      "        [28.9900],\n",
      "        [29.7200],\n",
      "        [30.1300],\n",
      "        [27.7800],\n",
      "        [30.4400],\n",
      "        [31.2600],\n",
      "        [29.2600],\n",
      "        [32.8900],\n",
      "        [34.5200],\n",
      "        [36.9100],\n",
      "        [35.1400],\n",
      "        [36.2400],\n",
      "        [37.0900],\n",
      "        [35.5500],\n",
      "        [27.2600],\n",
      "        [27.3900],\n",
      "        [22.0300],\n",
      "        [22.4300],\n",
      "        [25.8600],\n",
      "        [30.3000],\n",
      "        [32.8500],\n",
      "        [28.7500],\n",
      "        [29.5400],\n",
      "        [29.3900],\n",
      "        [30.6600],\n",
      "        [29.4300],\n",
      "        [30.0300],\n",
      "        [28.1100],\n",
      "        [30.8000],\n",
      "        [34.2300],\n",
      "        [33.2600],\n",
      "        [33.0600],\n",
      "        [36.8500],\n",
      "        [33.5000],\n",
      "        [35.7700],\n",
      "        [39.7700],\n",
      "        [36.9600],\n",
      "        [33.8300],\n",
      "        [32.6500],\n",
      "        [28.8100],\n",
      "        [27.0100],\n",
      "        [24.7500],\n",
      "        [25.1100],\n",
      "        [29.1900],\n",
      "        [29.9300],\n",
      "        [29.0500],\n",
      "        [26.3500],\n",
      "        [27.3800],\n",
      "        [26.2500],\n",
      "        [25.8300],\n",
      "        [32.6000],\n",
      "        [29.3900],\n",
      "        [29.5500],\n",
      "        [32.7400],\n",
      "        [32.1900],\n",
      "        [31.3600],\n",
      "        [33.9900],\n",
      "        [34.6100],\n",
      "        [35.2100],\n",
      "        [34.9900],\n",
      "        [39.8100],\n",
      "        [37.0400],\n",
      "        [35.1600],\n",
      "        [30.0300],\n",
      "        [24.4800],\n",
      "        [23.7300],\n",
      "        [29.2800],\n",
      "        [28.7000],\n",
      "        [32.8800],\n",
      "        [30.4000],\n",
      "        [28.5200],\n",
      "        [26.5800],\n",
      "        [22.6900],\n",
      "        [19.3800],\n",
      "        [26.8200],\n",
      "        [30.0500],\n",
      "        [30.1600],\n",
      "        [29.6900],\n",
      "        [30.1900],\n",
      "        [33.2500],\n",
      "        [33.5100],\n",
      "        [35.1100],\n",
      "        [37.0100],\n",
      "        [39.5400],\n",
      "        [38.2100],\n",
      "        [35.0800],\n",
      "        [34.5900],\n",
      "        [27.7100],\n",
      "        [19.9600],\n",
      "        [23.4600],\n",
      "        [19.8200],\n",
      "        [22.1000],\n",
      "        [27.7200],\n",
      "        [27.3400],\n",
      "        [27.7700],\n",
      "        [28.0300],\n",
      "        [28.1200],\n",
      "        [21.4400],\n",
      "        [23.2300],\n",
      "        [27.0100],\n",
      "        [29.2600],\n",
      "        [30.0700],\n",
      "        [32.2300],\n",
      "        [32.8000],\n",
      "        [34.5400],\n",
      "        [34.2400],\n",
      "        [35.6400],\n",
      "        [38.2400],\n",
      "        [37.7700],\n",
      "        [36.5200],\n",
      "        [31.0000],\n",
      "        [29.1000],\n",
      "        [28.2800],\n",
      "        [28.7400],\n",
      "        [23.3300],\n",
      "        [22.7500],\n",
      "        [26.5700],\n",
      "        [26.5200],\n",
      "        [25.0900],\n",
      "        [26.1500],\n",
      "        [17.5400],\n",
      "        [11.9600],\n",
      "        [22.6400],\n",
      "        [24.3100],\n",
      "        [26.8200],\n",
      "        [32.1300],\n",
      "        [30.3300],\n",
      "        [32.9500],\n",
      "        [34.8500],\n",
      "        [35.2800],\n",
      "        [35.0800],\n",
      "        [35.1800],\n",
      "        [38.7500],\n",
      "        [38.8400],\n",
      "        [35.6200],\n",
      "        [30.9300],\n",
      "        [31.6100],\n",
      "        [33.1200],\n",
      "        [24.5300],\n",
      "        [18.9200],\n",
      "        [21.1800],\n",
      "        [24.9600],\n",
      "        [24.9000],\n",
      "        [23.6700],\n",
      "        [21.3700],\n",
      "        [20.9400],\n",
      "        [22.6400],\n",
      "        [30.0000],\n",
      "        [31.3200],\n",
      "        [30.3800],\n",
      "        [33.1700],\n",
      "        [34.5700],\n",
      "        [35.4100],\n",
      "        [35.1800],\n",
      "        [36.1100],\n",
      "        [37.4800],\n",
      "        [38.7100],\n",
      "        [34.1800],\n",
      "        [35.9700],\n",
      "        [35.9900],\n",
      "        [37.2800],\n",
      "        [34.4700],\n",
      "        [29.9700],\n",
      "        [27.1700],\n",
      "        [30.1400],\n",
      "        [30.3300],\n",
      "        [29.5600],\n",
      "        [30.0400],\n",
      "        [29.2100],\n",
      "        [28.8100],\n",
      "        [28.9200],\n",
      "        [29.4100],\n",
      "        [28.6400],\n",
      "        [28.9400],\n",
      "        [30.4500],\n",
      "        [33.9300],\n",
      "        [35.8100],\n",
      "        [36.8600],\n",
      "        [42.4500],\n",
      "        [37.2900],\n",
      "        [37.0200],\n",
      "        [36.5900],\n",
      "        [31.3200],\n",
      "        [28.4500],\n",
      "        [26.6500],\n",
      "        [22.5500],\n",
      "        [20.6500],\n",
      "        [24.8800],\n",
      "        [29.4400],\n",
      "        [27.7600],\n",
      "        [29.4700],\n",
      "        [34.0800],\n",
      "        [26.2700],\n",
      "        [19.2300],\n",
      "        [20.9000],\n",
      "        [28.8200],\n",
      "        [31.6900],\n",
      "        [31.2300],\n",
      "        [29.9600],\n",
      "        [34.4500],\n",
      "        [33.6800],\n",
      "        [34.5900],\n",
      "        [37.9300],\n",
      "        [36.9900],\n",
      "        [36.3600],\n",
      "        [33.8200],\n",
      "        [31.8800],\n",
      "        [30.1700],\n",
      "        [28.2100],\n",
      "        [23.9200],\n",
      "        [24.4300],\n",
      "        [24.8700],\n",
      "        [30.2800],\n",
      "        [28.1400],\n",
      "        [28.7400],\n",
      "        [28.9500],\n",
      "        [16.7600],\n",
      "        [25.8600],\n",
      "        [29.3600],\n",
      "        [26.8500],\n",
      "        [27.4300],\n",
      "        [32.4000],\n",
      "        [31.3500],\n",
      "        [30.8900],\n",
      "        [34.6300],\n",
      "        [36.4300],\n",
      "        [37.9100],\n",
      "        [37.3100],\n",
      "        [36.6600],\n",
      "        [35.0300],\n",
      "        [31.3800],\n",
      "        [23.9600],\n",
      "        [25.7000],\n",
      "        [18.5800],\n",
      "        [16.4100],\n",
      "        [16.8900],\n",
      "        [18.9400],\n",
      "        [22.5900],\n",
      "        [29.5600],\n",
      "        [29.1300],\n",
      "        [25.8500],\n",
      "        [21.7800],\n",
      "        [27.9000],\n",
      "        [25.6300],\n",
      "        [29.6600],\n",
      "        [31.5600],\n",
      "        [31.8700],\n",
      "        [32.4100],\n",
      "        [31.9700],\n",
      "        [36.0500],\n",
      "        [39.2500],\n",
      "        [36.2400],\n",
      "        [32.7200],\n",
      "        [30.5800],\n",
      "        [24.0700],\n",
      "        [17.6500],\n",
      "        [33.6700],\n",
      "        [17.8400],\n",
      "        [15.6100],\n",
      "        [14.2100],\n",
      "        [14.3000],\n",
      "        [19.5400],\n",
      "        [18.5800],\n",
      "        [20.1400],\n",
      "        [18.5100],\n",
      "        [20.1700],\n",
      "        [25.8400],\n",
      "        [32.2900],\n",
      "        [30.5500],\n",
      "        [31.5900],\n",
      "        [30.9100],\n",
      "        [32.4700],\n",
      "        [36.1500],\n",
      "        [37.8800],\n",
      "        [37.5900],\n",
      "        [40.8400],\n",
      "        [38.0200],\n",
      "        [38.1900],\n",
      "        [33.8800],\n",
      "        [29.3700],\n",
      "        [27.6900],\n",
      "        [26.5600],\n",
      "        [26.2800],\n",
      "        [24.6900],\n",
      "        [30.4500],\n",
      "        [29.6800],\n",
      "        [27.0700],\n",
      "        [20.3800],\n",
      "        [12.5800],\n",
      "        [21.3000],\n",
      "        [24.5200],\n",
      "        [27.4400],\n",
      "        [29.7800],\n",
      "        [31.1000],\n",
      "        [33.1500],\n",
      "        [36.3000],\n",
      "        [34.2900],\n",
      "        [35.5900],\n",
      "        [36.8900],\n",
      "        [37.4400],\n",
      "        [36.8900],\n",
      "        [37.3100],\n",
      "        [37.6100],\n",
      "        [33.9000],\n",
      "        [35.7800],\n",
      "        [28.6300],\n",
      "        [23.4500],\n",
      "        [22.7000],\n",
      "        [25.1200],\n",
      "        [25.3400],\n",
      "        [29.1700],\n",
      "        [28.2000],\n",
      "        [24.8000],\n",
      "        [19.6100],\n",
      "        [22.6900],\n",
      "        [25.8100],\n",
      "        [30.0500],\n",
      "        [31.6300],\n",
      "        [31.4100],\n",
      "        [34.6800],\n",
      "        [34.6800],\n",
      "        [36.6100],\n",
      "        [37.0900],\n",
      "        [36.3500],\n",
      "        [36.7200],\n",
      "        [35.9200],\n",
      "        [35.3300],\n",
      "        [35.5000],\n",
      "        [36.5600],\n",
      "        [32.3900],\n",
      "        [30.6300],\n",
      "        [30.6500],\n",
      "        [30.8800],\n",
      "        [30.5800],\n",
      "        [29.4600],\n",
      "        [31.0100],\n",
      "        [32.1100],\n",
      "        [29.3300],\n",
      "        [35.1600],\n",
      "        [30.8100],\n",
      "        [33.0100],\n",
      "        [33.3800],\n",
      "        [33.8900],\n",
      "        [32.6200],\n",
      "        [35.1600],\n",
      "        [38.9600],\n",
      "        [39.2500],\n",
      "        [38.2600],\n",
      "        [38.2200],\n",
      "        [34.9200],\n",
      "        [32.3800],\n",
      "        [30.4000],\n",
      "        [27.0200],\n",
      "        [23.9000],\n",
      "        [26.0300],\n",
      "        [27.8900],\n",
      "        [30.5000],\n",
      "        [29.4800],\n",
      "        [29.3700],\n",
      "        [25.9200],\n",
      "        [26.8200],\n",
      "        [28.8900],\n",
      "        [29.4700],\n",
      "        [27.8700],\n",
      "        [28.9100],\n",
      "        [31.9500],\n",
      "        [32.0000],\n",
      "        [30.7400],\n",
      "        [33.1500],\n",
      "        [37.4300],\n",
      "        [38.2300],\n",
      "        [36.9800],\n",
      "        [38.2800],\n",
      "        [37.3500],\n",
      "        [32.8300],\n",
      "        [30.7800],\n",
      "        [27.3600],\n",
      "        [28.5000],\n",
      "        [26.3500],\n",
      "        [26.9400],\n",
      "        [26.7700],\n",
      "        [28.2200],\n",
      "        [27.4300],\n",
      "        [29.8300],\n",
      "        [26.6900],\n",
      "        [31.2500],\n",
      "        [30.4400],\n",
      "        [31.7300],\n",
      "        [32.4700],\n",
      "        [30.3300],\n",
      "        [30.1500],\n",
      "        [33.5800],\n",
      "        [34.1500],\n",
      "        [34.7400],\n",
      "        [35.8800],\n",
      "        [40.2800],\n",
      "        [37.7400],\n",
      "        [35.3400],\n",
      "        [33.3000],\n",
      "        [28.6000],\n",
      "        [23.2300],\n",
      "        [24.3800],\n",
      "        [22.9200],\n",
      "        [23.8700],\n",
      "        [31.1500],\n",
      "        [29.9700],\n",
      "        [29.3000],\n",
      "        [27.6100],\n",
      "        [27.0500],\n",
      "        [22.2500],\n",
      "        [25.7300],\n",
      "        [28.4000],\n",
      "        [30.1600],\n",
      "        [30.8200],\n",
      "        [31.7700],\n",
      "        [32.0200],\n",
      "        [34.6200],\n",
      "        [34.0500],\n",
      "        [35.1300],\n",
      "        [40.2300],\n",
      "        [38.2000],\n",
      "        [35.2800],\n",
      "        [32.0500],\n",
      "        [25.5600],\n",
      "        [25.9600],\n",
      "        [27.2900],\n",
      "        [20.7000],\n",
      "        [20.9100],\n",
      "        [24.7200],\n",
      "        [24.5100],\n",
      "        [27.3000],\n",
      "        [29.9300],\n",
      "        [23.6500],\n",
      "        [12.4600],\n",
      "        [19.7100],\n",
      "        [24.9700],\n",
      "        [28.9500],\n",
      "        [27.3900],\n",
      "        [29.2300],\n",
      "        [32.5600],\n",
      "        [33.2100],\n",
      "        [34.7600],\n",
      "        [36.7800],\n",
      "        [36.5800],\n",
      "        [39.2400],\n",
      "        [36.3700],\n",
      "        [34.0100],\n",
      "        [29.7500],\n",
      "        [25.6200],\n",
      "        [25.1400],\n",
      "        [21.8500],\n",
      "        [24.9700],\n",
      "        [28.0900],\n",
      "        [25.7600],\n",
      "        [24.0400],\n",
      "        [26.8800],\n",
      "        [16.8600],\n",
      "        [11.7400],\n",
      "        [15.8300],\n",
      "        [21.9800],\n",
      "        [25.2300],\n",
      "        [31.0500],\n",
      "        [31.1700],\n",
      "        [32.1800],\n",
      "        [36.6000],\n",
      "        [35.8300],\n",
      "        [37.8100],\n",
      "        [36.1600],\n",
      "        [37.9700],\n",
      "        [36.7100],\n",
      "        [36.0400],\n",
      "        [35.5200],\n",
      "        [31.5400],\n",
      "        [28.5300],\n",
      "        [29.8100],\n",
      "        [26.9600],\n",
      "        [25.7300],\n",
      "        [25.3800],\n",
      "        [23.4300],\n",
      "        [19.9000],\n",
      "        [22.6500],\n",
      "        [21.5900],\n",
      "        [16.9800],\n",
      "        [26.2100],\n",
      "        [31.1600],\n",
      "        [31.6700],\n",
      "        [31.6700],\n",
      "        [33.7300],\n",
      "        [35.0200],\n",
      "        [35.4200],\n",
      "        [38.7000],\n",
      "        [38.5400],\n",
      "        [37.9500],\n",
      "        [38.5800],\n",
      "        [37.5300],\n",
      "        [38.9200],\n",
      "        [34.6500],\n",
      "        [32.7000],\n",
      "        [28.7800],\n",
      "        [29.8000],\n",
      "        [31.2200],\n",
      "        [29.4600],\n",
      "        [28.8800],\n",
      "        [27.0800],\n",
      "        [32.4600],\n",
      "        [32.4300],\n",
      "        [32.2300],\n",
      "        [27.5300],\n",
      "        [27.3700],\n",
      "        [30.8100],\n",
      "        [31.7900],\n",
      "        [31.6700],\n",
      "        [33.5000],\n",
      "        [34.9000],\n",
      "        [34.7200],\n",
      "        [38.7600],\n",
      "        [38.1500],\n",
      "        [36.5200],\n",
      "        [31.2900],\n",
      "        [26.6300],\n",
      "        [25.3700],\n",
      "        [25.2600],\n",
      "        [21.2400],\n",
      "        [23.3300],\n",
      "        [27.3400],\n",
      "        [30.5100],\n",
      "        [28.3200],\n",
      "        [23.7000]]), None))\n",
      "Batch length: 2\n",
      "Element 0: <class 'dict'>\n",
      "Element 1: <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 배치 확인\n",
    "batch = next(iter(test_dataloader))\n",
    "print(batch)\n",
    "\n",
    "# 배치가 튜플인 경우\n",
    "if isinstance(batch, tuple):\n",
    "    print(f\"Batch length: {len(batch)}\")\n",
    "    for i, element in enumerate(batch):\n",
    "        print(f\"Element {i}: {type(element)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
